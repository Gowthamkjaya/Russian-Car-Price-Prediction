{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd180a1d",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-28T01:44:07.966737Z",
     "iopub.status.busy": "2025-05-28T01:44:07.966490Z",
     "iopub.status.idle": "2025-05-28T01:44:09.335486Z",
     "shell.execute_reply": "2025-05-28T01:44:09.334267Z"
    },
    "papermill": {
     "duration": 1.375112,
     "end_time": "2025-05-28T01:44:09.336781",
     "exception": false,
     "start_time": "2025-05-28T01:44:07.961669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/russian-car-plates-prices-prediction/sample_submission.csv\n",
      "/kaggle/input/russian-car-plates-prices-prediction/supplemental_english.py\n",
      "/kaggle/input/russian-car-plates-prices-prediction/supplemental_russian.py\n",
      "/kaggle/input/russian-car-plates-prices-prediction/train.csv\n",
      "/kaggle/input/russian-car-plates-prices-prediction/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa8c123c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:44:09.344685Z",
     "iopub.status.busy": "2025-05-28T01:44:09.344303Z",
     "iopub.status.idle": "2025-05-28T01:44:11.338025Z",
     "shell.execute_reply": "2025-05-28T01:44:11.337407Z"
    },
    "papermill": {
     "duration": 1.998714,
     "end_time": "2025-05-28T01:44:11.339322",
     "exception": false,
     "start_time": "2025-05-28T01:44:09.340608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter\n",
    "import holidays\n",
    "from supplemental_english import GOVERNMENT_CODES, REGION_CODES \n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, KBinsDiscretizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc618e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:44:11.346063Z",
     "iopub.status.busy": "2025-05-28T01:44:11.345750Z",
     "iopub.status.idle": "2025-05-28T01:44:11.460089Z",
     "shell.execute_reply": "2025-05-28T01:44:11.459290Z"
    },
    "papermill": {
     "duration": 0.118778,
     "end_time": "2025-05-28T01:44:11.461149",
     "exception": false,
     "start_time": "2025-05-28T01:44:11.342371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- XGBoost with Enhanced Features, Robust FE & CV---\n",
      "\n",
      "1. Load Data and Initial Exploration\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"--- XGBoost with Enhanced Features, Robust FE & CV---\")\n",
    "\n",
    "print(\"\\n1. Load Data and Initial Exploration\\n\" + \"=\"*50)\n",
    "# Load original training data to derive global statistics\n",
    "df_train_orig = pd.read_csv(r'/kaggle/input/russian-car-plates-prices-prediction/train.csv')\n",
    "df_test_orig = pd.read_csv(r'/kaggle/input/russian-car-plates-prices-prediction/test.csv')\n",
    "\n",
    "# Define global statistics from the original training set\n",
    "global_mean_price = df_train_orig['price'].mean()\n",
    "global_median_price = df_train_orig['price'].median()\n",
    "global_std_price = df_train_orig['price'].std()\n",
    "global_mean_price_log = np.log1p(global_mean_price)\n",
    "global_median_price_log = np.log1p(global_median_price)\n",
    "\n",
    "# Concatenate for consistent feature engineering\n",
    "df_train_orig['is_train'] = 1\n",
    "df_test_orig['is_train'] = 0 # Price column will be NaN here\n",
    "df = pd.concat([df_train_orig, df_test_orig], ignore_index=True)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "region_lookup = {}\n",
    "for region, codes in REGION_CODES.items():\n",
    "    for code in codes:\n",
    "        region_lookup[str(code)] = region # Ensure codes are strings for lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1419900f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:44:11.467714Z",
     "iopub.status.busy": "2025-05-28T01:44:11.467504Z",
     "iopub.status.idle": "2025-05-28T01:44:11.485304Z",
     "shell.execute_reply": "2025-05-28T01:44:11.484399Z"
    },
    "papermill": {
     "duration": 0.022542,
     "end_time": "2025-05-28T01:44:11.486595",
     "exception": false,
     "start_time": "2025-05-28T01:44:11.464053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Feature Engineering Functions ---\n",
    "def parse_plate_components(plate):\n",
    "    plate = str(plate).upper().strip()\n",
    "    match = re.match(r'^([ABEKMHOPCTYX])(\\d{1,3})([ABEKMHOPCTYX]{2})(\\d{2,3})$', plate)\n",
    "    if match:\n",
    "        first_letter = match.group(1)\n",
    "        numbers_str = match.group(2).zfill(3) # Pad numbers to 3 digits\n",
    "        last_letters_str = match.group(3)\n",
    "        region_code_str = match.group(4)\n",
    "        full_letters = first_letter + last_letters_str\n",
    "        return full_letters, numbers_str, region_code_str, first_letter, last_letters_str\n",
    "\n",
    "    # Fallback for less standard formats or if the above doesn't match perfectly\n",
    "    letters_found = ''.join(re.findall(r'[ABEKMHOPCTYX]', plate))\n",
    "    numbers_found = ''.join(re.findall(r'\\d+', plate)) # Find all digit sequences\n",
    "\n",
    "    parsed_letters = letters_found if letters_found else ''\n",
    "    \n",
    "    if len(numbers_found) >= 5: # e.g., 12377 or 123777\n",
    "        if numbers_found[-3:].isdigit() and (len(numbers_found) - 3) > 0 : # Check for 3-digit region\n",
    "             parsed_numbers = numbers_found[:-3].zfill(3)[-3:] # take last 3 of number part\n",
    "             parsed_region = numbers_found[-3:]\n",
    "        elif numbers_found[-2:].isdigit() and (len(numbers_found) - 2) > 0 : # Check for 2-digit region\n",
    "             parsed_numbers = numbers_found[:-2].zfill(3)[-3:]\n",
    "             parsed_region = numbers_found[-2:]\n",
    "        else: # Default to taking first 3 as number, rest as region if possible\n",
    "            parsed_numbers = numbers_found[:3].zfill(3)\n",
    "            parsed_region = numbers_found[3:][-3:] if len(numbers_found[3:]) >=2 else numbers_found[3:]\n",
    "\n",
    "    elif len(numbers_found) == 3 or len(numbers_found) == 4: # e.g. 123 or 1237 (assume 123 is number, 7 is bad region)\n",
    "        parsed_numbers = numbers_found[:3].zfill(3)\n",
    "        parsed_region = numbers_found[3:] if len(numbers_found) > 3 else '' # Region only if extra digits\n",
    "    elif len(numbers_found) > 0 : # Less than 3 digits, assume all are numbers\n",
    "        parsed_numbers = numbers_found.zfill(3)[-3:]\n",
    "        parsed_region = ''\n",
    "    else:\n",
    "        parsed_numbers = '000'\n",
    "        parsed_region = ''\n",
    "        \n",
    "    _first_letter = parsed_letters[0] if parsed_letters else ''\n",
    "    _last_letters = parsed_letters[1:] if len(parsed_letters) > 1 else '' # Simplified, actual last two are harder if format varies\n",
    "\n",
    "    return parsed_letters, parsed_numbers, parsed_region, _first_letter, _last_letters\n",
    "\n",
    "\n",
    "def get_government_significance_enhanced(letters, numbers_str, region_code_str):\n",
    "    if not letters and not numbers_str and not region_code_str:\n",
    "        return False, 0, 0, 0, \"Non-governmental\"\n",
    "    try: numbers_int = int(numbers_str) # numbers_str should already be 3 digits\n",
    "    except ValueError: numbers_int = -1\n",
    "    \n",
    "    # Ensure letters is at least 1 char for gov_letters[0] access\n",
    "    if not letters: \n",
    "        return False, 0, 0, 0, \"Non-governmental\"\n",
    "\n",
    "    for (gov_letters, num_range, gov_region), details in GOVERNMENT_CODES.items():\n",
    "        current_gov_first_letter = gov_letters[0] if isinstance(gov_letters, tuple) else gov_letters[0]\n",
    "        if (letters == gov_letters and \n",
    "            num_range[0] <= numbers_int <= num_range[1] and \n",
    "            str(region_code_str) == str(gov_region)):\n",
    "            agency, forbidden, advantage, significance_val = details[0], details[1], details[2], details[3]\n",
    "            return True, forbidden, advantage, significance_val, agency\n",
    "    return False, 0, 0, 0, \"Non-governmental\"\n",
    "\n",
    "\n",
    "def categorize_agency(agency_desc):\n",
    "    agency_desc = str(agency_desc)\n",
    "    if agency_desc == 'Non-governmental': return 'Non-governmental'\n",
    "    if 'President' in agency_desc: return 'Presidential'\n",
    "    if 'Police' in agency_desc.lower() or 'Internal Affairs' in agency_desc: return 'Police/Security'\n",
    "    if 'Government' in agency_desc and 'Federation Council' not in agency_desc and 'State Duma' not in agency_desc: return 'Government'\n",
    "    if 'Military' in agency_desc or 'Army' in agency_desc or 'Defense' in agency_desc: return 'Military'\n",
    "    if 'Federal' in agency_desc or 'FSB' in agency_desc or 'FSO' in agency_desc: return 'Federal Services'\n",
    "    if 'Judge' in agency_desc or 'Court' in agency_desc or 'Justice' in agency_desc or 'prosecutor' in agency_desc.lower(): return 'Judicial'\n",
    "    if 'Administration' in agency_desc: return 'Administration'\n",
    "    if 'Diplomatic' in agency_desc: return 'Diplomatic'\n",
    "    return 'Other Governmental'\n",
    "\n",
    "def analyze_number_patterns_enhanced(numbers_str):\n",
    "    numbers_str = str(numbers_str).zfill(3) # Ensure it's a 3-digit string\n",
    "    digit_counts = Counter(numbers_str)\n",
    "    max_repeat = 0\n",
    "    if numbers_str.isdigit() and digit_counts: # Check if digit_counts is not empty\n",
    "         max_repeat = max(digit_counts.values())\n",
    "    \n",
    "    is_sequential, is_reverse_seq, is_palindrome_flag, has_mirror_effect, is_low_number_flag = False, False, False, False, False\n",
    "    if numbers_str.isdigit() and len(numbers_str) == 3 : \n",
    "        is_sequential = any(numbers_str[i:i+2] in '01234567890' for i in range(len(numbers_str)-1)) or \\\n",
    "                        any(numbers_str[i:i+3] in '012345678901' for i in range(len(numbers_str)-2))\n",
    "        is_reverse_seq = any(numbers_str[i:i+2] in '98765432109' for i in range(len(numbers_str)-1)) or \\\n",
    "                         any(numbers_str[i:i+3] in '987654321098' for i in range(len(numbers_str)-2))\n",
    "        is_palindrome_flag = numbers_str == numbers_str[::-1]\n",
    "        has_mirror_effect = (numbers_str[0] == numbers_str[-1]) or is_palindrome_flag\n",
    "        try: \n",
    "            is_low_number_flag = int(numbers_str) < 100\n",
    "        except ValueError:\n",
    "            is_low_number_flag = False\n",
    "    return max_repeat, is_sequential, is_reverse_seq, is_palindrome_flag, is_low_number_flag, has_mirror_effect\n",
    "\n",
    "def enrich_date_features(df_to_enrich):\n",
    "    df_to_enrich['year'] = df_to_enrich['date'].dt.year\n",
    "    df_to_enrich['month'] = df_to_enrich['date'].dt.month\n",
    "    df_to_enrich['day'] = df_to_enrich['date'].dt.day\n",
    "    df_to_enrich['day_of_week'] = df_to_enrich['date'].dt.dayofweek\n",
    "    df_to_enrich['week_of_year'] = df_to_enrich['date'].dt.isocalendar().week.astype(int)\n",
    "    df_to_enrich['quarter'] = df_to_enrich['date'].dt.quarter\n",
    "    if not df_to_enrich['date'].empty and df_to_enrich['date'].notna().all():\n",
    "         df_to_enrich['total_days'] = (df_to_enrich['date'] - df_to_enrich['date'].min()).dt.days\n",
    "    else:\n",
    "         df_to_enrich['total_days'] = 0 \n",
    "         if df_to_enrich['date'].isna().any():\n",
    "             print(\"Warning: NaNs found in 'date' column during total_days calculation. Filling with 0.\")\n",
    "    \n",
    "    df_to_enrich['day_name'] = df_to_enrich['date'].dt.day_name() # ADDED day_name\n",
    "\n",
    "    df_to_enrich['is_weekend'] = df_to_enrich['date'].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "    df_to_enrich['weekday_sin'] = np.sin(2 * np.pi * df_to_enrich['day_of_week'] / 7)\n",
    "    df_to_enrich['weekday_cos'] = np.cos(2 * np.pi * df_to_enrich['day_of_week'] / 7)\n",
    "    df_to_enrich['day_sin'] = np.sin(2 * np.pi * df_to_enrich['day'] / 31) \n",
    "    df_to_enrich['day_cos'] = np.cos(2 * np.pi * df_to_enrich['day'] / 31)\n",
    "    df_to_enrich['month_sin'] = np.sin(2 * np.pi * df_to_enrich['month'] / 12)\n",
    "    df_to_enrich['month_cos'] = np.cos(2 * np.pi * df_to_enrich['month'] / 12)\n",
    "    \n",
    "    unique_years = df_to_enrich['year'].dropna().unique() \n",
    "    if len(unique_years) > 0 :\n",
    "        try:\n",
    "            ru_holidays = holidays.Russia(years=unique_years.astype(int).tolist()) \n",
    "            df_to_enrich['is_holiday'] = df_to_enrich['date'].apply(lambda x: x.date() in ru_holidays if pd.notna(x) else False).astype(int)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not generate holiday features: {e}. Setting 'is_holiday' to 0.\")\n",
    "            df_to_enrich['is_holiday'] = 0 \n",
    "    else:\n",
    "        df_to_enrich['is_holiday'] = 0\n",
    "    return df_to_enrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e0da9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:44:11.493885Z",
     "iopub.status.busy": "2025-05-28T01:44:11.493310Z",
     "iopub.status.idle": "2025-05-28T01:44:14.692496Z",
     "shell.execute_reply": "2025-05-28T01:44:14.691872Z"
    },
    "papermill": {
     "duration": 3.204046,
     "end_time": "2025-05-28T01:44:14.693741",
     "exception": false,
     "start_time": "2025-05-28T01:44:11.489695",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Applying Feature Engineering...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Apply Feature Engineering ---\n",
    "print(\"\\n2. Applying Feature Engineering...\\n\" + \"=\"*50)\n",
    "plate_components_extracted = df['plate'].apply(parse_plate_components)\n",
    "df['letters'] = [comp[0] for comp in plate_components_extracted]\n",
    "df['numbers'] = [comp[1] for comp in plate_components_extracted] \n",
    "df['region_code'] = [comp[2] for comp in plate_components_extracted]\n",
    "df['first_letter'] = [comp[3] for comp in plate_components_extracted] # ADDED\n",
    "df['last_letters'] = [comp[4] for comp in plate_components_extracted] # ADDED\n",
    "\n",
    "df['numbers'] = df['numbers'].fillna('000').str.zfill(3) \n",
    "df['region_code'] = df['region_code'].fillna('0').astype(str) # Ensure string for lookup\n",
    "df['letters'] = df['letters'].fillna('')\n",
    "df['first_letter'] = df['first_letter'].fillna('') # ADDED\n",
    "df['last_letters'] = df['last_letters'].fillna('') # ADDED\n",
    "df['region_name'] = df['region_code'].map(lambda x: region_lookup.get(str(x), \"UnknownRegion\"))\n",
    "\n",
    "gov_features = df.apply(lambda row: get_government_significance_enhanced(row['letters'], row['numbers'], row['region_code']), axis=1)\n",
    "df['is_govt_plate'] = [feat[0] for feat in gov_features]\n",
    "df['forbidden_to_buy'] = [feat[1] for feat in gov_features]\n",
    "df['advantage_on_road'] = [feat[2] for feat in gov_features]\n",
    "df['significance'] = [feat[3] for feat in gov_features]\n",
    "df['agency'] = [feat[4] for feat in gov_features]\n",
    "df['agency_category'] = df['agency'].apply(categorize_agency)\n",
    "\n",
    "df = enrich_date_features(df) \n",
    "\n",
    "prestigious_numbers_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 111, 222, 333, 444, 555, 666, 777, 888, 999, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
    "prestigious_letters_list = [\"AAA\", \"MMM\", \"EEE\", \"KKK\", \"OOO\", \"PPP\", \"CCC\", \"TTT\", \"XXX\"] \n",
    "df['is_prestigious_number'] = df['numbers'].apply(lambda x: 1 if x.isdigit() and int(x) in prestigious_numbers_list else 0)\n",
    "df['is_prestigious_letters'] = df['letters'].apply(lambda x: 1 if x in prestigious_letters_list else 0)\n",
    "\n",
    "pattern_features = df['numbers'].apply(analyze_number_patterns_enhanced)\n",
    "df['max_repeated_digits'] = [feat[0] for feat in pattern_features]\n",
    "df['is_sequential_num'] = [feat[1] for feat in pattern_features] \n",
    "df['is_reverse_sequential_num'] = [feat[2] for feat in pattern_features] \n",
    "df['is_palindrome_num'] = [feat[3] for feat in pattern_features] \n",
    "df['is_low_number'] = [feat[4] for feat in pattern_features]\n",
    "df['has_mirror_numbers'] = [feat[5] for feat in pattern_features]\n",
    "\n",
    "df['unique_letters_count'] = df['letters'].apply(lambda x: len(set(x)) if isinstance(x, str) and x else 0)\n",
    "df['has_repeated_letters'] = df['letters'].apply(lambda x: len(set(x)) < len(x) if isinstance(x, str) and x else False)\n",
    "\n",
    "numbers_freq_map = df['numbers'].value_counts(normalize=True).to_dict()\n",
    "df['numbers_freq_enc'] = df['numbers'].map(numbers_freq_map).fillna(0)\n",
    "df['numbers_log_freq_enc'] = np.log1p(df['numbers_freq_enc'])\n",
    "\n",
    "train_df_for_encoding = df[df['is_train'] == 1].copy()\n",
    "\n",
    "# Target Encoding for letters, first_letter, last_letters\n",
    "if 'price' in train_df_for_encoding.columns and not train_df_for_encoding['price'].isnull().all():\n",
    "    letter_mean_price_map = train_df_for_encoding.groupby('letters')['price'].mean().apply(np.log1p).to_dict()\n",
    "    df['letters_mean_price_log'] = df['letters'].map(letter_mean_price_map)\n",
    "    \n",
    "    first_letter_mean_price_map = train_df_for_encoding.groupby('first_letter')['price'].mean().apply(np.log1p).to_dict() # ADDED\n",
    "    df['first_letter_mean_price_log'] = df['first_letter'].map(first_letter_mean_price_map) # ADDED\n",
    "    \n",
    "    last_letters_mean_price_map = train_df_for_encoding.groupby('last_letters')['price'].mean().apply(np.log1p).to_dict() # ADDED\n",
    "    df['last_letters_mean_price_log'] = df['last_letters'].map(last_letters_mean_price_map) # ADDED\n",
    "else:\n",
    "    df['letters_mean_price_log'] = np.nan \n",
    "    df['first_letter_mean_price_log'] = np.nan # ADDED\n",
    "    df['last_letters_mean_price_log'] = np.nan # ADDED\n",
    "\n",
    "df['letters_mean_price_log'] = df['letters_mean_price_log'].fillna(global_mean_price_log)\n",
    "df['first_letter_mean_price_log'] = df['first_letter_mean_price_log'].fillna(global_mean_price_log) # ADDED\n",
    "df['last_letters_mean_price_log'] = df['last_letters_mean_price_log'].fillna(global_mean_price_log) # ADDED\n",
    "\n",
    "\n",
    "df['region_avg_price'] = np.nan\n",
    "df['region_median_price'] = np.nan\n",
    "df['region_price_std'] = np.nan\n",
    "df['region_count'] = np.nan\n",
    "\n",
    "if ('price' in train_df_for_encoding.columns and \\\n",
    "    not train_df_for_encoding['price'].isnull().all() and \\\n",
    "    'region_name' in train_df_for_encoding.columns and \\\n",
    "    not train_df_for_encoding['region_name'].isnull().all() and \\\n",
    "    len(train_df_for_encoding) > 0):\n",
    "    try:\n",
    "        region_stats = train_df_for_encoding.groupby('region_name')['price'].agg(['mean', 'median', 'std', 'count']).reset_index()\n",
    "        region_stats.columns = ['region_name', 'temp_region_avg_price', 'temp_region_median_price', 'temp_region_price_std', 'temp_region_count']\n",
    "        if not region_stats.empty:\n",
    "            df = df.merge(region_stats, on='region_name', how='left')\n",
    "            df['region_avg_price'] = df['temp_region_avg_price'] \n",
    "            df['region_median_price'] = df['temp_region_median_price']\n",
    "            df['region_price_std'] = df['temp_region_price_std']\n",
    "            df['region_count'] = df['temp_region_count']\n",
    "            df.drop(columns=['temp_region_avg_price', 'temp_region_median_price', \n",
    "                             'temp_region_price_std', 'temp_region_count'], inplace=True, errors='ignore')\n",
    "    except Exception as e:\n",
    "        print(f\"Error during region_stats calculation or merge: {e}. Region stats will use global defaults.\")\n",
    "else:\n",
    "    print(\"Warning: Insufficient data for region_stats. Using global defaults.\")\n",
    "\n",
    "df['region_avg_price'] = df['region_avg_price'].fillna(global_mean_price)\n",
    "df['region_median_price'] = df['region_median_price'].fillna(global_median_price)\n",
    "df['region_price_std'] = df['region_price_std'].fillna(global_std_price if pd.notna(global_std_price) else 0)\n",
    "df['region_count'] = df['region_count'].fillna(1)\n",
    "\n",
    "df['region_avg_price_log'] = np.log1p(df['region_avg_price'])\n",
    "df['region_median_price_log'] = np.log1p(df['region_median_price'])\n",
    "df['region_avg_price_log'] = df['region_avg_price_log'].fillna(global_mean_price_log)\n",
    "df['region_median_price_log'] = df['region_median_price_log'].fillna(global_median_price_log)\n",
    "\n",
    "premium_regions_list = ['Moscow', 'Saint Petersburg', 'Moscow Oblast', 'Leningrad Oblast', 'Republic of Tatarstan', 'Sverdlovsk Oblast', 'Krasnodar Krai']\n",
    "df['is_premium_region'] = df['region_name'].isin(premium_regions_list).astype(int)\n",
    "\n",
    "df = df.sort_values(['plate', 'date']) \n",
    "df['price_lag_1'] = df.groupby('plate')['price'].shift(1) \n",
    "df['plate_listing_count'] = df.groupby('plate').cumcount() + 1\n",
    "df['price_lag_1'] = df['price_lag_1'].fillna(global_median_price) \n",
    "df['price_lag_1_log'] = np.log1p(df['price_lag_1'])\n",
    "\n",
    "df['prestige_score'] = ((df['is_prestigious_letters'].astype(int) * 3) + \\\n",
    "                        (df['is_prestigious_number'].astype(int) * 2) + \\\n",
    "                        (df['has_repeated_letters'].astype(int) * 1) + \\\n",
    "                        (df['has_mirror_numbers'].astype(int) * 1) + \\\n",
    "                        (df['is_palindrome_num'].astype(int) *1) + \\\n",
    "                        (df['is_sequential_num'].astype(int) * 1) + \\\n",
    "                        (df['significance'].astype(int))) \n",
    "df['prestige_rank'] = df['prestige_score'].rank(method='average', pct=True)\n",
    "\n",
    "df['letters_region_combo'] = df['letters'].astype(str) + \"_\" + df['region_code'].astype(str)\n",
    "letters_region_freq_map = df['letters_region_combo'].value_counts(normalize=True).to_dict()\n",
    "df['letters_region_freq'] = df['letters_region_combo'].map(letters_region_freq_map).fillna(0)\n",
    "\n",
    "df['is_gov_and_prestige'] = df['is_govt_plate'].astype(int) * df['prestige_score'].astype(int)\n",
    "\n",
    "df['letters_fillna'] = df['letters'].fillna('') \n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 2), min_df=5, max_features=100) \n",
    "letter_text_features = vectorizer.fit_transform(df['letters_fillna'])\n",
    "letter_text_features_df = pd.DataFrame(letter_text_features.toarray(), columns=[f\"letter_vec_{i}\" for i in vectorizer.get_feature_names_out()])\n",
    "df = pd.concat([df.reset_index(drop=True), letter_text_features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "df.loc[df['is_train'] == 1, 'price_log'] = np.log1p(df.loc[df['is_train'] == 1, 'price'])\n",
    "df['price_log'] = df['price_log'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796768e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:44:14.700641Z",
     "iopub.status.busy": "2025-05-28T01:44:14.700321Z",
     "iopub.status.idle": "2025-05-28T01:44:14.834177Z",
     "shell.execute_reply": "2025-05-28T01:44:14.833315Z"
    },
    "papermill": {
     "duration": 0.138719,
     "end_time": "2025-05-28T01:44:14.835463",
     "exception": false,
     "start_time": "2025-05-28T01:44:14.696744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Categorical columns for Ordinal Encoding: 9 (['agency', 'agency_category', 'day_name', 'first_letter', 'last_letters']...)\n",
      "Final Numerical columns for Imputation/Scaling: 138 (['advantage_on_road', 'day', 'day_cos', 'day_of_week', 'day_sin']...)\n"
     ]
    }
   ],
   "source": [
    "# --- Define Feature Columns ---\n",
    "base_feature_cols = [\n",
    "    'letters', 'numbers', 'region_code', 'region_name', 'agency', 'agency_category', 'day_name', # Added day_name\n",
    "    'first_letter', 'last_letters', # ADDED\n",
    "    'is_govt_plate', 'forbidden_to_buy', 'advantage_on_road', 'significance', \n",
    "    'is_prestigious_number', 'is_prestigious_letters', 'prestige_score', 'prestige_rank', \n",
    "    'max_repeated_digits', 'is_sequential_num', 'is_reverse_sequential_num', 'is_palindrome_num', \n",
    "    'is_low_number', 'has_repeated_letters', 'unique_letters_count', 'has_mirror_numbers', \n",
    "    'year', 'month', 'day', 'day_of_week', 'week_of_year', 'quarter', 'total_days', 'is_weekend', 'is_holiday', \n",
    "    'weekday_sin', 'weekday_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos', \n",
    "    'numbers_freq_enc', 'numbers_log_freq_enc', \n",
    "    'letters_mean_price_log', \n",
    "    'first_letter_mean_price_log', 'last_letters_mean_price_log', # ADDED\n",
    "    'region_avg_price_log', 'region_median_price_log', 'region_price_std', 'region_count', 'is_premium_region', \n",
    "    'price_lag_1_log', 'plate_listing_count', \n",
    "    'letters_region_freq', \n",
    "    'is_gov_and_prestige'\n",
    "]\n",
    "text_feature_cols_final = [col for col in df.columns if col.startswith(\"letter_vec_\")] \n",
    "feature_cols = sorted(list(set(base_feature_cols + text_feature_cols_final)))\n",
    "feature_cols = [col for col in feature_cols if col in df.columns]\n",
    "\n",
    "\n",
    "# Separate train and test\n",
    "train_df = df[df['is_train'] == 1].copy()\n",
    "test_df = df[df['is_train'] == 0].copy()\n",
    "\n",
    "# --- Data for Models ---\n",
    "X = train_df[feature_cols].copy() \n",
    "y_log = train_df['price_log'].copy()\n",
    "y_original = train_df['price'].copy() \n",
    "X_test_payload_original_features = test_df[feature_cols].copy()\n",
    "\n",
    "\n",
    "# --- SMAPE Metric ---\n",
    "def calculate_smape(y_true_orig, y_pred_orig):\n",
    "    y_true_orig = np.array(y_true_orig)\n",
    "    y_pred_orig = np.array(y_pred_orig)\n",
    "    y_pred_orig = np.maximum(y_pred_orig, 0) \n",
    "    numerator = np.abs(y_true_orig - y_pred_orig)\n",
    "    denominator = (np.abs(y_true_orig) + np.abs(y_pred_orig) + 1e-8) \n",
    "    return np.mean(2 * numerator / denominator) * 100\n",
    "\n",
    "# --- Preprocessing ---\n",
    "categorical_cols_str = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
    "\n",
    "final_cat_cols = [col for col in categorical_cols_str if col in X.columns]\n",
    "final_num_cols = [col for col in numerical_cols if col in X.columns and col not in final_cat_cols]\n",
    "\n",
    "print(f\"Final Categorical columns for Ordinal Encoding: {len(final_cat_cols)} ({final_cat_cols[:5]}...)\") # Print first 5\n",
    "print(f\"Final Numerical columns for Imputation/Scaling: {len(final_num_cols)} ({final_num_cols[:5]}...)\")\n",
    "\n",
    "\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing_value_placeholder')), # More specific fill_value\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=np.float32))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, final_num_cols),\n",
    "        ('cat', categorical_transformer, final_cat_cols)\n",
    "    ],\n",
    "    remainder='drop' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e372ae5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:44:14.842337Z",
     "iopub.status.busy": "2025-05-28T01:44:14.842105Z",
     "iopub.status.idle": "2025-05-28T01:48:21.172804Z",
     "shell.execute_reply": "2025-05-28T01:48:21.172046Z"
    },
    "papermill": {
     "duration": 246.335642,
     "end_time": "2025-05-28T01:48:21.174288",
     "exception": false,
     "start_time": "2025-05-28T01:44:14.838646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost with enhanced features...\n",
      "  Fold 1/10\n",
      "    Fold SMAPE: 35.2761\n",
      "  Fold 2/10\n",
      "    Fold SMAPE: 34.7346\n",
      "  Fold 3/10\n",
      "    Fold SMAPE: 34.6729\n",
      "  Fold 4/10\n",
      "    Fold SMAPE: 35.3771\n",
      "  Fold 5/10\n",
      "    Fold SMAPE: 35.9054\n",
      "  Fold 6/10\n",
      "    Fold SMAPE: 35.3538\n",
      "  Fold 7/10\n",
      "    Fold SMAPE: 35.2809\n",
      "  Fold 8/10\n",
      "    Fold SMAPE: 35.6031\n",
      "  Fold 9/10\n",
      "    Fold SMAPE: 35.2824\n",
      "  Fold 10/10\n",
      "    Fold SMAPE: 36.2005\n",
      "\n",
      "  XGBoost Average CV SMAPE (all features): 35.3687\n"
     ]
    }
   ],
   "source": [
    "# --- Model Training and Cross-Validation ---\n",
    "N_SPLITS = 10 \n",
    "y_bins = KBinsDiscretizer(n_bins=N_SPLITS, encode='ordinal', strategy='quantile', subsample=None) \n",
    "y_log_binned = y_bins.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n",
    "kf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "xgb_params = {\n",
    "    'n_estimators': 10000, \n",
    "    'max_depth': 10, \n",
    "    'learning_rate': 0.01852160907217988,\n",
    "    'subsample': 0.6786672470738663,\n",
    "    'colsample_bytree': 0.46208650739218005,\n",
    "    'reg_alpha': 0.017519138973638618,\n",
    "    'reg_lambda': 0.2839310763317462,\n",
    "    'gamma': 0.0033995958574628547,\n",
    "    'objective': 'reg:tweedie', \n",
    "    'tweedie_variance_power': 1.0869464555654937, \n",
    "    'random_state': 42, \n",
    "    'n_jobs': -1, \n",
    "    'tree_method': 'hist',\n",
    "    'early_stopping_rounds': 100 \n",
    "}\n",
    "\n",
    "model_name = 'XGBoost'\n",
    "xgb_oof_preds_log = np.zeros(X.shape[0])\n",
    "xgb_test_preds_log_sum = np.zeros(X_test_payload_original_features.shape[0])\n",
    "xgb_fold_scores = []\n",
    "xgb_feature_importances_list = []\n",
    "\n",
    "print(f\"\\nTraining {model_name} with enhanced features...\")\n",
    "for fold_idx, (train_idx, val_idx) in enumerate(kf.split(X, y_log_binned)):\n",
    "    print(f\"  Fold {fold_idx+1}/{N_SPLITS}\")\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold_log, y_val_fold_log = y_log.iloc[train_idx], y_log.iloc[val_idx]\n",
    "    y_val_fold_orig = y_original.iloc[val_idx] \n",
    "\n",
    "    preprocessor_fitted = preprocessor.fit(X_train_fold)\n",
    "    X_train_fold_processed = preprocessor_fitted.transform(X_train_fold)\n",
    "    X_val_fold_processed = preprocessor_fitted.transform(X_val_fold)\n",
    "    X_test_fold_processed = preprocessor_fitted.transform(X_test_payload_original_features)\n",
    "\n",
    "    current_model = xgb.XGBRegressor(**xgb_params) \n",
    "    current_model.fit(X_train_fold_processed, y_train_fold_log,\n",
    "                      eval_set=[(X_val_fold_processed, y_val_fold_log)],\n",
    "                      verbose=False) \n",
    "    \n",
    "    fold_val_pred_log = current_model.predict(X_val_fold_processed)\n",
    "    fold_test_pred_log = current_model.predict(X_test_fold_processed)\n",
    "    \n",
    "    processed_feature_names_for_fold = preprocessor_fitted.get_feature_names_out()\n",
    "    fold_importances = pd.Series(current_model.feature_importances_, index=processed_feature_names_for_fold)\n",
    "    xgb_feature_importances_list.append(fold_importances)\n",
    "\n",
    "    xgb_oof_preds_log[val_idx] = fold_val_pred_log\n",
    "    xgb_test_preds_log_sum += fold_test_pred_log\n",
    "\n",
    "    fold_val_pred_orig = np.expm1(fold_val_pred_log)\n",
    "    fold_smape = calculate_smape(y_val_fold_orig, fold_val_pred_orig)\n",
    "    xgb_fold_scores.append(fold_smape)\n",
    "    print(f\"    Fold SMAPE: {fold_smape:.4f}\")\n",
    "\n",
    "xgb_avg_cv_smape = np.mean(xgb_fold_scores)\n",
    "print(f\"\\n  {model_name} Average CV SMAPE (all features): {xgb_avg_cv_smape:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2e53c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-28T01:48:21.182015Z",
     "iopub.status.busy": "2025-05-28T01:48:21.181763Z",
     "iopub.status.idle": "2025-05-28T01:48:21.210456Z",
     "shell.execute_reply": "2025-05-28T01:48:21.209661Z"
    },
    "papermill": {
     "duration": 0.033908,
     "end_time": "2025-05-28T01:48:21.211714",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.177806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Top 20 XGBoost Features (averaged over folds):\n",
      "num__prestige_score            0.088663\n",
      "num__prestige_rank             0.071089\n",
      "num__is_prestigious_letters    0.042509\n",
      "num__unique_letters_count      0.025870\n",
      "num__is_prestigious_number     0.024011\n",
      "num__letters_mean_price_log    0.021013\n",
      "num__numbers_freq_enc          0.020575\n",
      "num__significance              0.020538\n",
      "num__price_lag_1_log           0.019583\n",
      "num__numbers_log_freq_enc      0.018200\n",
      "num__advantage_on_road         0.017567\n",
      "num__year                      0.017231\n",
      "num__is_gov_and_prestige       0.016129\n",
      "cat__region_code               0.015352\n",
      "num__letter_vec_mp             0.013067\n",
      "num__max_repeated_digits       0.012493\n",
      "num__letter_vec_am             0.011018\n",
      "num__region_avg_price_log      0.010459\n",
      "num__region_count              0.010369\n",
      "cat__agency_category           0.009383\n",
      "dtype: float32\n",
      "\n",
      "Submission file created successfully: submission.csv\n",
      "       id         price\n",
      "6   54874  1.033723e+06\n",
      "12  52711  3.859312e+05\n",
      "26  52799  2.333992e+05\n",
      "27  53569  1.813600e+06\n",
      "36  57371  5.317144e+05\n"
     ]
    }
   ],
   "source": [
    "avg_xgb_importances = pd.Series(dtype='float64') \n",
    "if xgb_feature_importances_list:\n",
    "    try:\n",
    "        common_index = xgb_feature_importances_list[0].index\n",
    "        for fi_series in xgb_feature_importances_list[1:]:\n",
    "            common_index = common_index.intersection(fi_series.index)\n",
    "        \n",
    "        reindexed_importances = [fi.reindex(common_index).fillna(0) for fi in xgb_feature_importances_list]\n",
    "        if reindexed_importances:\n",
    "             avg_xgb_importances = pd.concat(reindexed_importances, axis=1).mean(axis=1).sort_values(ascending=False)\n",
    "        else: # Should not happen if xgb_feature_importances_list is not empty\n",
    "            avg_xgb_importances = pd.Series(dtype='float64')\n",
    "\n",
    "\n",
    "        print(f\"\\n  Top 20 {model_name} Features (averaged over folds):\")\n",
    "        print(avg_xgb_importances.head(20))\n",
    "    except Exception as e:\n",
    "        print(f\"Could not average XGBoost importances due to: {e}.\")\n",
    "        if xgb_feature_importances_list: \n",
    "             avg_xgb_importances = xgb_feature_importances_list[-1].sort_values(ascending=False)\n",
    "\n",
    "\n",
    "xgb_final_test_preds_log = xgb_test_preds_log_sum / N_SPLITS\n",
    "xgb_final_test_preds_orig = np.expm1(xgb_final_test_preds_log)\n",
    "xgb_final_test_preds_orig = np.maximum(xgb_final_test_preds_orig, 0) \n",
    "\n",
    "submission = pd.DataFrame({'id': test_df['id'], 'price': xgb_final_test_preds_orig}) \n",
    "submission_filename = 'submission.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "print(f\"\\nSubmission file created successfully: {submission_filename}\")\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6788b6e",
   "metadata": {
    "papermill": {
     "duration": 0.002938,
     "end_time": "2025-05-28T01:48:21.218091",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.215153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542746a9",
   "metadata": {
    "papermill": {
     "duration": 0.003487,
     "end_time": "2025-05-28T01:48:21.224555",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.221068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acb1064",
   "metadata": {
    "papermill": {
     "duration": 0.002827,
     "end_time": "2025-05-28T01:48:21.230378",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.227551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33823dac",
   "metadata": {
    "papermill": {
     "duration": 0.003107,
     "end_time": "2025-05-28T01:48:21.236486",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.233379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c4e13",
   "metadata": {
    "papermill": {
     "duration": 0.002752,
     "end_time": "2025-05-28T01:48:21.242254",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.239502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6b218",
   "metadata": {
    "papermill": {
     "duration": 0.002743,
     "end_time": "2025-05-28T01:48:21.247859",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.245116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c42b94",
   "metadata": {
    "papermill": {
     "duration": 0.002759,
     "end_time": "2025-05-28T01:48:21.253531",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.250772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c82e12",
   "metadata": {
    "papermill": {
     "duration": 0.002813,
     "end_time": "2025-05-28T01:48:21.259329",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.256516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576279a",
   "metadata": {
    "papermill": {
     "duration": 0.002811,
     "end_time": "2025-05-28T01:48:21.265098",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.262287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93cbfc7",
   "metadata": {
    "papermill": {
     "duration": 0.00277,
     "end_time": "2025-05-28T01:48:21.270866",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.268096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2768d",
   "metadata": {
    "papermill": {
     "duration": 0.003011,
     "end_time": "2025-05-28T01:48:21.276905",
     "exception": false,
     "start_time": "2025-05-28T01:48:21.273894",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11239181,
     "sourceId": 94521,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 257.75542,
   "end_time": "2025-05-28T01:48:21.898196",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-28T01:44:04.142776",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
